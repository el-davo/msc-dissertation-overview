\begin{abstract}
Micro-services are currently the hot new technology for the web, they allow us to break up our otherwise monolithic architecture into much smaller more focused services. This has a lot of benefits, such as reducing system size and complexity and increasing release frequency and agility. 

There is one flaw with a micro-services architecture which I would like to attempt to address in this paper. When a particular micro service crashes it can be hard to find the root cause. Typically a developer would start their analysis by checking the logs of the failed service. This can be both time consuming and potentially lead to misdiagnoses. The reason is that the developer is not seeing the full picture. For example, the failure on service A could be a direct result of a problem that originated on service B. It is also possible that the root cause could be concealing itself in some convoluted log message that a typical developer could misinterpret. Due to the nature of micro-services encouraging continuous deployment it is also possible that a crash was as a direct result of a service deployment at a particular point in time. 

This paper will focus mainly on root cause analysis of micro-services deployed to a popular PaaS called Pivotal Cloud Foundry

\end{abstract}

\chapter{Research Context}
\lhead{\emph{Research Context}}
In my current organization, we have a micro-service architecture. There are 100+ services written in multiple languages, including Node.js, Java, Python, Golang. We deploy all of our services onto Pivotal Cloud Foundry. Our current solution for root cause analysis is an ELK stack (Elastic-search, Log-stash and, Kibana). This provides a nice interface to see the logs from all services. While an ELK stack can provide some interesting insights into your environment such as geo identification of web users\cite{7508191}, it has some drawbacks in it being a very manual process. You need to manually identify what times certain events took place and manually enter your own queries, which is very time-consuming.

To have an effective root cause analysis system we will need to look at certain metrics in real time. Log analysis has become a key metric over the last few years, however, with the emergence of micro-services, it has become more challenging in the following ways.

\begin{itemize}
  \item Logs from multiple services written in different languages. i.e. Node.js or Java
  \item Log streaming and log analysis in real time. 
  \item Store and process potentially Gigabytes of log data safely
  \item Auto-scaling
  \item Continuous deployments
\end{itemize}

There has been research done in the past which tries to solve these challenges by utilizing Big Data analysis with Apache Spark and machine learning techniques\cite{8067504}. Similarly, this paper\cite{7748933} proposes to use Apache Flume and Apache Spark for real-time analysis.

For the machine learning aspects, I would need a dataset to compare against. Usually, when a developer has a problem they cannot solve they turn to a website called stackoverflow.com. This is an answer and questions website for developers where users can post a question containing a stack trace and the community would give them answers on what the potential cause is, the better the answer the more ratings it has. This could be a potential source of data sets for machine learning. It is possible to mine a website such as stackoverflow as is described in "Association rule mining for finding usability problem patterns"\cite{8320144}.

While log analysis should play a valuable role in our root cause analysis, I would like to take this further and utilize some of the features that Cloud Foundry can give us.

Pivotal Cloud Foundry (PCF) has become a popular platform as a service (PaaS) over the last few years\cite{6924673}. It makes working with micro-services much easier with features including zero downtime blue/green deployments, auto-scaling and access to a multitude of third-party services like MySQL and RabbitMQ

PCF has a rich public API which can give us a lot of extra data on the state of our environment, for example it is possible to interrogate the core language a service was written in by interrogating build-pack types, if we know the language it may make log analysis easier as, we can, in theory, apply a more targeted set of rules to our analysis for that specific language. PCF can also give us current memory and disk space usage of deployed applications as well as events that took place within a certain time-frame such as starting, stopping or crashing.

Given all of the above I believe this research to be worthwhile. If done effectively the results would be highly beneficial to my current organization, and to any other organization deploying micro-services to Pivotal Cloud Foundry.